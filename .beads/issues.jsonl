{"id":"model_guided_research-03v","title":"Make default install CPU-friendly (move JAX CUDA deps to optional extras)","description":"Current default deps require CUDA JAX (jax[cuda12]) even for CPU-only users/tests. Make base install CPU-friendly (jax>=...) and move CUDA plugin deps into optional extras; update README install/troubleshooting accordingly; regenerate uv.lock.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-12-17T22:59:50.186027747Z","updated_at":"2025-12-17T23:09:41.293704741Z","closed_at":"2025-12-17T23:09:41.293704741Z","close_reason":"Completed: base deps are CPU-friendly (jax>=...), cuda12 extra added, uv.lock regenerated.","compaction_level":0}
{"id":"model_guided_research-0fw","title":"Numerical stability & NaN/Inf watchpoints","description":"- Add optional asserts/logging hooks for NaN/Inf detection across demos and nanochat trainers (leveraging config.check_numerics, torch.autograd anomaly if needed) with rich summaries.\n- Provide guidelines when to enable, and sample outputs; keep default off.\n- Ensure no performance hit when disabled.","acceptance_criteria":"Optional NaN/Inf detection hooks wired (config + torch/JAX toggles); sample output shown; default off; negligible overhead when off; guidance on when to enable; no false positives in normal runs.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:46.840801328Z","updated_at":"2025-12-18T05:42:04.856737611Z","closed_at":"2025-12-18T05:42:04.856737611Z","close_reason":"Completed: optional NaN/Inf watchpoints wired for JAX demos + nanochat (loss/grad checks, anomaly flag), rich diagnostics + docs guidance; default off; tests pass.","compaction_level":0}
{"id":"model_guided_research-0hu","title":"CMA-ES objective validation tests","description":"- Add quick unit/integration checks for the CMA-ES objective wrapper: deterministic seed run, sanity loss range, variance tolerance, and toy Rosenbrock-style test to catch regressions.\n- Wire into CI-lite script optionally; no heavy training.\n- Document how to run locally.","acceptance_criteria":"Tests or scripts that validate CMA-ES objective end-to-end on small config plus toy function; pass/fail documented; runnable via uv/CI-lite; avoids long GPU runs.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:34:00.108006736Z","updated_at":"2025-12-18T07:01:59.428742578Z","closed_at":"2025-12-18T07:01:59.428742578Z","close_reason":"Added CMA-ES objective validation tests: tests/test_cmaes_objective.py includes a Rosenbrock CMA ask/tell sanity check and an end-to-end deterministic tiny nanochat.train run using a locally generated tiny parquet dataset under a temp NANOCHAT_BASE_DIR (no network). Verified with uv run pytest tests/test_cmaes_objective.py -q.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-0hu","depends_on_id":"model_guided_research-ajh","type":"blocks","created_at":"2025-11-20T08:34:00.114909832Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-0jk","title":"Validate GPU correctness + perf after FlexAttention","description":"- Run correctness checks comparing FlexAttention path vs standard attention on representative configs (including synaptic/tropical/ultrametric variants) using deterministic seeds; assert close outputs.\n- Benchmark throughput, memory, and latency on GPU; capture tokens/s and TFLOP/s with and without FlexAttention under same FLOPs budget defined in baseline task.\n- Record results in artifacts/perf/flex_vs_standard.md with tables and commands; note hardware/driver versions.\n- Re-run unit/CI tests with flex flag gated for torch>=2.5; ensure failures are addressed.","acceptance_criteria":"Correctness: Flex vs standard attention outputs match within tolerance on seeded batches across key configs (synaptic/tropical/ultrametric variants). Performance: tokens/s and TFLOP/s reported side-by-side; memory peak captured. Runs executed on GPU with same FLOPs budget as baseline. Results written to artifacts/perf/flex_vs_standard.{json,md} with hardware/driver versions. All tests green or noted with rationale.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:06:54.791643891Z","updated_at":"2025-12-18T04:24:59.578625980Z","closed_at":"2025-12-18T04:24:59.578625980Z","close_reason":"Correctness verified (standard+synaptic) and perf report written to artifacts/perf/flex_vs_standard.{md,json}; tests pass.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-0jk","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:06:54.808266553Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-0jk","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:06:54.809643358Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-0nf","title":"CMA-ES budget & scheduling plan","description":"- Translate the bio-inspired cost/timeline template to this repo with realistic GPU availability; define phased budget (min/target/max) and scheduling assumptions (parallel GPUs, wall-clock) for Phase 1 and optional Phase 2.\n- Document in docs/cmaes_budget_schedule.md; include triggers for pause/continue.","acceptance_criteria":"Budget/schedule doc published with min/target/max costs, GPU assumptions, duration per phase, pause/go triggers; aligned with harness and proxy objective plans.","status":"closed","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:32:33.060085866Z","updated_at":"2025-12-18T09:27:57.530607479Z","closed_at":"2025-12-18T09:27:57.530607479Z","close_reason":"Added docs/cmaes_budget_schedule.md with Phase 1/2 min/target/max eval budgets, scheduling assumptions (single vs multi-GPU), and pause/go triggers; aligned with existing CMA-ES plan/proxy beads.","compaction_level":0}
{"id":"model_guided_research-0tt","title":"Nanochat ultrametric: integrate trie/packed-prefix lookup (beyond O(T^2) kernel)","description":"Current ultrametric attention in nanochat is an O(T^2) LCP-kernel (continuous relaxation). Implement the trie-based LCP-Tree Attention (LTA) lookup/update path described in markdown_documentation/ultrametric_worlds_and_p_adic_computation.md, leveraging the packed trie layout work in model_guided_research-a1o. Keep kernel mode as a baseline/fallback.","acceptance_criteria":"Trie/packed lookup path exists and is selectable; KV-cache semantics preserved; outputs match kernel/reference within tolerance on small seeds; perf sanity check shows reduced overhead vs O(T^2) for longer T; documented limits.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-18T08:20:19.299915022Z","updated_at":"2025-12-18T10:31:32.150382771Z","closed_at":"2025-12-18T10:31:32.150382771Z","close_reason":"Added selectable ultrametric trie mode for KV-cache decode (CPU-only): packed prefix trie with subtree sums/counts; added GPTConfig/train flags; parity test vs hard-digit kernel; perf sanity helper; kept kernel fallback.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-0tt","depends_on_id":"model_guided_research-3bx","type":"discovered-from","created_at":"2025-12-18T08:20:19.305105931Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-0tt","depends_on_id":"model_guided_research-a1o","type":"blocks","created_at":"2025-12-18T08:20:19.306674967Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-0wn","title":"CMA-ES result analysis & sensitivity","description":"- After pilot, compute parameter importance (OAT or covariance analysis), summarize correlations, and produce actionable defaults.\n- Save sensitivity ranking and correlation heatmaps to artifacts/cmaes/analysis/.\n- Feed findings into config templates and meta-eval planning.","acceptance_criteria":"Analysis note with ranked sensitivities/correlations produced from CMA-ES runs; plots/JSON saved under artifacts/cmaes/analysis/; actionable defaults or follow-up recs listed.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:32:15.328410890Z","updated_at":"2025-11-20T08:32:15.328410890Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-0wn","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:32:15.329807191Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-1ix","title":"GPU-scale benchmark runs with proper profiling","description":"Run benchmarks on GPU with realistic budgets: (1) Target 1e12+ FLOPs runs (not just smoke tests), (2) Enable CUDA profiling for kernel analysis, (3) Compare FlexAttention vs SDPA vs exotic attention, (4) Measure actual TFLOPs achieved, (5) Document GPU environment for reproducibility. Current runs are mostly CPU smoke tests.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-12-19T20:56:01.818952917Z","updated_at":"2025-12-19T21:01:09.638051310Z","closed_at":"2025-12-19T21:01:09.638051310Z","close_reason":"Not a separate task - GPU is just a CLI flag (--device cuda). GPU runs should be part of z7r benchmarking, not a separate bead.","compaction_level":0}
{"id":"model_guided_research-1j5","title":"CMA-ES small-model transfer study","description":"- Evaluate optimizing hyperparams on a tiny model (few layers) then transferring to larger configs; measure transfer success vs overfit-to-scale.\n- Plan experiment design and cost; optional pilot if cheap.\n- Document results/feasibility.","acceptance_criteria":"Design written for small-to-large transfer; if run, includes pilot data; otherwise scoped with clear go/no-go triggers; stored with cmaes_plan_mgr addendum.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:34:30.731490489Z","updated_at":"2025-11-20T08:34:30.731490489Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-1j5","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:34:30.743645490Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-1ra","title":"Activation checkpointing strategy","description":"- Define when/how to use activation checkpointing in nanochat to trade memory vs compute; note interaction with FlexAttention and reversible modules.\n- Provide recommended configs and estimated savings; ensure compatibility with profiling hooks.\n- Document only; no code changes yet.","acceptance_criteria":"Checkpointing strategy doc compares memory/time tradeoffs for standard vs reversible vs Flex; recommended toggles and expected savings; verified compatibility with profiler hooks; no code regressions.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:43.353096558Z","updated_at":"2025-11-20T08:13:26.211289788Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-1ra","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:12:04.099634877Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-1ra","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:10:43.354867960Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-23h","title":"Nanochat braid: discrete decoder + optional YBE-valid crossing law","description":"Extend nanochat/braid_attention_torch.py beyond the current soft sigmoid interaction: add an opt-in discrete braid-word decoder (permutation + allowed crossings), local verification hooks, and (optionally) a Yang–Baxter-satisfying crossing law option (depends on model_guided_research-k2y).","acceptance_criteria":"Braid attention supports a discrete decoding mode producing a braid-word (or equivalent schedule) with invariant checks; optional YBE-valid crossing law available; tests verify causality + basic invariant preservation; documented scope/limitations.","status":"closed","priority":3,"issue_type":"feature","assignee":"","created_at":"2025-12-18T08:20:11.867149864Z","updated_at":"2025-12-18T11:20:12.408599506Z","closed_at":"2025-12-18T11:20:12.408599506Z","close_reason":"Implemented opt-in discrete braid decode mode in nanochat with recorded per-head schedule + invariant checks (KV-cache decode), added braid_mode/tau/crossing_law flags (incl. YBE law), wired through GPTConfig + nanochat.train, and added tests for KV-cache consistency + schedule recording.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-23h","depends_on_id":"model_guided_research-3bx","type":"discovered-from","created_at":"2025-12-18T08:20:11.872197694Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-23h","depends_on_id":"model_guided_research-k2y","type":"blocks","created_at":"2025-12-18T08:20:11.873817335Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-23x","title":"Cross-repo idea bank & transfer plan","description":"- Mine bio_inspired_nanochat for techniques (FlexAttention usage, torch.compile settings, optimizers/schedulers, dataloader/cache, profiling habits, Triton/Rust plans) and map each to this repo with feasibility/effort notes.\n- Produce a living doc (docs/cross_repo_ideas.md) with links to files/lines and proposed integration path or rationale for not porting.\n- Highlight quick wins vs longer-term items; assign owners where possible.","acceptance_criteria":"Doc exists with ≥10 concrete ideas mapped from bio_inspired_nanochat to this repo, each with file/line refs, feasibility, proposed next action; at least 3 marked as quick-win candidates; stored at docs/cross_repo_ideas.md.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:17:33.820400395Z","updated_at":"2025-11-20T08:17:33.820400395Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-23x","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:17:48.487921945Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-28n","title":"Documentation sync: CLI help & examples","description":"- Ensure CLI flags (mgr run, nanochat train scripts) are documented with current options (Flex, profiling, FLOPs harness) in README or separate doc.\n- Add concise examples for common tasks: run demo, baseline train, Flex on/off, benchmark harness, profiler traces.\n- Avoid duplicating content; link where possible.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:56.343948839Z","updated_at":"2025-12-18T07:04:54.760938130Z","closed_at":"2025-12-18T07:04:54.760938130Z","close_reason":"Updated README.md CLI usage with a new Benchmarks (fixed budgets) section documenting fixed-FLOPs nanochat runs (nanochat.train --target-flops), the mgr bench-fixed-flops suite command, and mgr eval (practical utility) with artifacts output.","compaction_level":0}
{"id":"model_guided_research-2bx","title":"Nanochat config templates for fair comparisons","description":"- Provide ready-to-run config snippets (JSON/TOML/md) for: vanilla baseline, FlexAttention on/off, math-demo hybrid runs; annotate FLOPs, seq length, batch size.\n- Store under configs/ or docs/config_examples.md; ensure they align with benchmark harness.\n- No code changes; focus on clarity and parity.","acceptance_criteria":"Config templates (baseline, Flex on/off, math hybrid) published under configs/ or docs with batch/seq/model, FLOPs estimate, feature flags, seed; validated to parse or be referenced by harness; clearly versioned.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:42.499509628Z","updated_at":"2025-11-20T08:13:19.776492101Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2bx","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:11:53.556932380Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2cg","title":"Per-op microbench (attention/MLP) vs vanilla","description":"- Design microbenchmarks for critical ops (attention variants, MLP variants) comparing math-enhanced vs vanilla implementations at controlled shapes (batch, seq, head_dim) and dtypes.\n- Measure latency, GB/s, TFLOP/s, L2 cache hit estimations where possible; optionally use torch.utils.benchmark or custom timers.\n- Ensure runs isolated from full training loop; include CPU/GPU variants where relevant.","acceptance_criteria":"At least two op types (one attention, one MLP/activation) benchmarked math vs vanilla at ≥2 shape settings; results saved to artifacts/microbench/ with timing tables and commands; notes on measurement methodology included.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:14:38.016787889Z","updated_at":"2025-11-20T08:14:38.016787889Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2cg","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:15:01.796546388Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-2cg","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:15:04.098201395Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2co","title":"CMA-ES Phase 2 (expanded param groups)","description":"- Plan follow-on CMA-ES runs over expanded parameter subsets (e.g., calcium/vesicle/structural analogues) or math-demo-specific knobs; contingent on Phase 1 success.\n- Define grouping, budgets, and sequencing; integrate with artifacts/telemetry.\n- No execution until Go from Phase 1.","acceptance_criteria":"Phase 2 plan documented (which groups, bounds, budgets, sequence, stopping rules) and linked to Phase 1 outcomes; ready to trigger once Phase 1 success criteria are met.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:32:05.274599462Z","updated_at":"2025-11-20T08:32:05.274599462Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2co","depends_on_id":"model_guided_research-0nf","type":"blocks","created_at":"2025-11-20T08:32:44.059358730Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-2co","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:32:05.287466470Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2e5","title":"Integrate FlexAttention path into nanochat stack","description":"- Port SynapticFlexAttention-style implementation to current nanochat attention modules (gpt.py/gpt_synaptic.py, synaptic.py, tropical/ultrametric/etc. if applicable) behind a flag  with safe fallback.\n- Handle masking, dropout, causal/noncausal options consistent with existing math features; ensure compatibility with training scripts and tests.\n- Provide microbench/correctness checks mirroring bio_inspired_nanochat/scripts/verify_flex_correctness.py and benchmark_flex.py; add CI-friendly toggles to skip if torch<2.5.\n- Ensure no feature regression vs documented math (keep rich diagnostics, comments intact).","acceptance_criteria":"FlexAttention path lands behind flag (e.g., use_flex_attention) with safe fallback; supports causal/noncausal + masking; passes correctness parity tests vs standard attention on representative configs; works with torch>=2.5 else clean skip; rich logging preserved; no regressions in existing tests; doc snippet added pointing to usage/flags.","status":"closed","priority":1,"issue_type":"feature","assignee":"","created_at":"2025-11-20T08:06:49.662099463Z","updated_at":"2025-12-18T02:09:51.838679047Z","closed_at":"2025-12-18T02:09:51.838679047Z","close_reason":"Completed: standard GPT + synaptic FlexAttention behind flags; per-layer synaptic KV state; correctness+benchmark scripts; docs snippet; tests green","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2e5","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:06:49.676541684Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2kz","title":"Dataloader performance tuning","description":"- Profile nanochat dataloader throughput (PyTorch + JAX pathways) for target batch/seq; check pin_memory, num_workers, prefetch, shared memory contention.\n- Recommend tuned defaults or CLI flags for GPU runs; align with bio_inspired_nanochat if better.\n- Document findings; no code deletions.","acceptance_criteria":"Dataloader profiling completed with recommended num_workers/pin_memory/prefetch for target GPU; before/after throughput numbers recorded; optional flags documented; no regressions in correctness.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:49.656513208Z","updated_at":"2025-11-20T08:13:16.469095395Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2kz","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:12:07.207539801Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2l8","title":"Gauge demo: add BCH/Magnus fusion mini-experiment (theory→code)","description":"matrix_exponential_gauge_learning.md discusses collapsing multiple generator steps via BCH/Magnus-style fusion as an inference/compression path. The current Gauge-Transformer demo includes expmv sequence mixing + structured channel blocks but does not implement an end-to-end fusion experiment. Add a small, self-contained fusion micro-demo: take a sequence of small generators, form a fused generator approximation (e.g., 1st/2nd order BCH), compare exp(action) vs product(action) error, and report when fusion is safe (commutator norm thresholds).","acceptance_criteria":"Fusion micro-demo exists (CPU/JAX) with clear metrics (error vs commutator norms) and docs guidance; does not change default model behavior; produces a concise rich output table.","status":"closed","priority":3,"issue_type":"task","assignee":"","created_at":"2025-12-18T07:42:32.954378988Z","updated_at":"2025-12-19T04:12:03.123834910Z","closed_at":"2025-12-19T04:12:03.123834910Z","close_reason":"Implemented BCH/Magnus fusion micro-demo with error/commutator metrics and docs guidance","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2l8","depends_on_id":"model_guided_research-3fi","type":"discovered-from","created_at":"2025-12-18T07:42:32.981621687Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2mj","title":"CMA-ES budget/auto-stop guard","description":"- Implement budget guardrails: max GPU-hours/evals, no-improvement patience, and graceful early-stop with checkpoint.\n- Expose config flags/env; log when stopping; avoid runaway cost.\n- Integrate with regression gate/reporting.","acceptance_criteria":"Configurable budget/patience guard implemented; triggers early-stop with checkpoint and clear message; defaults documented; sample run demonstrating stop condition saved to artifacts/cmaes/.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:34:06.990433525Z","updated_at":"2025-11-20T08:34:06.990433525Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2mj","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:34:06.992122624Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2vs","title":"Artifacts index & manifest","description":"- Generate an index/manifest file summarizing available artifacts (baseline, bench, perf, vis, reports) with paths, timestamps, config hashes, and brief metric highlights.\n- Update after each run or via a manual command; keep in artifacts/index.json(+md).\n- Helps discover latest baselines/benchmarks for dashboards/regression checks.","acceptance_criteria":"Manifest generator produces index.json (and optional markdown) listing artifacts with metadata (run id/commit, config hash, metrics highlights, paths); stored under artifacts/; sample run executed.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:20:10.127178133Z","updated_at":"2025-11-20T08:20:10.127178133Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2vs","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:20:13.224287997Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-2xa","title":"Rust/PyO3 kernel parity study","description":"- Review bio_inspired_nanochat rust_src and PyO3 bindings plan; list which ops could map to Rust kernels here (metrics, genetics, structural fused?).\n- Assess feasibility vs current JAX/Torch implementations; note build/toolchain requirements.\n- Produce recommendation note (docs/rust_kernel_options.md) outlining next steps; no code changes now.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:15.034017493Z","updated_at":"2025-11-20T08:09:15.034017493Z","compaction_level":0}
{"id":"model_guided_research-2xy","title":"CMA-ES early-stop/cheap proxy objective","description":"- Design fast proxy objective for CMA-ES (short training, extrapolated curves, or slim model) to cut cost; define extrapolation or multi-stage scoring.\n- Validate proxy vs full run on small sample; document correlation and acceptable error.\n- Integrate into cmaes_plan_mgr doc.","acceptance_criteria":"Proxy objective defined with budget, extrapolation/scoring method, and correlation check; small validation comparing proxy vs full recorded; documented in cmaes_plan_mgr or dedicated note; ready for use in Phase 1/2.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:32:10.232381687Z","updated_at":"2025-11-20T08:32:10.232381687Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-2xy","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:32:48.385003664Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-2xy","depends_on_id":"model_guided_research-ajh","type":"blocks","created_at":"2025-11-20T08:32:10.243346592Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-3bx","title":"Ensure nanochat implementations match documented math (no simplifications)","description":"- Review nanochat attention/optimizer/scheduler code (synaptic, braid, tropical, ultrametric, simplicial, quaternion/octonion, ordinal) to confirm full feature set matches README/markdown docs; identify any 'toy' or placeholder code paths.\n- Restore or add back rich inline comments where removed; cite rationale if any are obsolete.\n- Note required follow-up changes as sub-tasks or comments; avoid deleting files.\n- Produce summary note with file/line references for missing pieces to guide implementations.","acceptance_criteria":"Audit completed confirming implementations match documented math; list of gaps + fixes required; no unjustified simplifications remain; blockers noted; feeds into comment restoration task.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:07:04.896589068Z","updated_at":"2025-12-18T08:21:55.756027919Z","closed_at":"2025-12-18T08:21:55.756027919Z","close_reason":"Completed nanochat docs↔impl alignment pass: tropical/ultrametric semantics fixed, simplicial KV-cache parity fixed, audit report written (docs/nanochat_alignment_audit.md), and follow-up beads created for remaining gaps.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-3bx","depends_on_id":"model_guided_research-3fi","type":"blocks","created_at":"2025-11-20T08:11:43.812774963Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-3fi","title":"Audit demos vs docs; restore full richness/comments","description":"- Cross-read each demo .py against its matching markdown_documentation file to ensure every documented feature is present (no simplified placeholders).\n- Restore explanatory comments removed in prior edits when still applicable; avoid net loss of useful commentary.\n- Produce a per-module checklist covering matrix-gauge, ultrametric, simplicial, nonstandard, octonion, ordinal, reversible, ifs-fractal, knot-braid, surreal, tropical; note gaps.\n- Deliver a short markdown report (e.g., docs/alignment_audit.md) summarizing findings; code changes limited to comment restoration.","acceptance_criteria":"Per-module checklist completed (code vs doc); missing features/comments identified; initial comment restorations done where applicable; summary report saved (docs/alignment_audit.md).","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:06:34.584326173Z","updated_at":"2025-12-18T07:44:03.002596886Z","closed_at":"2025-12-18T07:44:03.002596886Z","close_reason":"Completed docs↔demos checklist (docs/alignment_audit.md), restored/added explanatory comments (docs pointers + scope notes), and created follow-up beads for remaining gaps.","compaction_level":0,"comments":[{"id":3,"issue_id":"model_guided_research-3fi","author":"ubuntu","text":"Completed docs↔demos audit and added docs pointers/comments in all 11 demos; wrote checklist + gap notes in . Created follow-up beads:  (ultrametric packed trie layout),  (YBE crossing-law option),  (Gauge BCH/Magnus fusion micro-demo). Validation: ................................................                         [100%]\n=============================== warnings summary ===============================\ntests/test_demos.py::test_nanochat_gpt_synaptic_flexattention_smoke\ntests/test_demos.py::test_nanochat_gpt_synaptic_flexattention_smoke\n  /data/projects/model_guided_research/.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:1114: RuntimeWarning: Guards may run slower on Python 3.13.0. Consider upgrading to Python 3.13.1+.\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n48 passed, 2 warnings in 13.48s (48 passed).","created_at":"2025-12-18T07:43:44Z"},{"id":4,"issue_id":"model_guided_research-3fi","author":"ubuntu","text":"Correction (shell quoting): audit output is in docs/alignment_audit.md. Follow-up beads created: model_guided_research-a1o (ultrametric packed trie layout), model_guided_research-k2y (YBE crossing-law option for braid demo), model_guided_research-2l8 (Gauge BCH/Magnus fusion micro-demo). Validation: uv run pytest tests/test_demos.py -q (48 passed; torch/dynamo Python 3.13 guard warning only).","created_at":"2025-12-18T07:43:58Z"}]}
{"id":"model_guided_research-4p5","title":"Benchmark results comparison dashboard","description":"Rich CLI dashboard for comparing benchmark runs: (1) Side-by-side attention type comparison tables, (2) Sparklines for loss curves, (3) Memory and throughput comparisons, (4) Statistical significance highlights, (5) Best configuration recommendations. Makes results accessible without manual JSON parsing.","status":"closed","priority":2,"issue_type":"feature","assignee":"","created_at":"2025-12-19T20:56:18.871927796Z","updated_at":"2025-12-19T21:01:07.183495386Z","closed_at":"2025-12-19T21:01:07.183495386Z","close_reason":"Redundant with existing nyp (Rich training dashboard). Dashboard comparison can be part of nyp.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-4p5","depends_on_id":"model_guided_research-e8k","type":"blocks","created_at":"2025-12-19T20:56:35.292165594Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-5bo","title":"Establish vanilla nanochat baseline (fixed FLOPs)","description":"- Define comparable training/FLOPs budget (e.g., token count * model FLOPs per token) and log config used.\n- Add or reuse a script/notebook to run baseline nanochat training with that budget on available GPU, capturing throughput (tokens/s), loss curve, memory, TFLOP/s.\n- Export metrics/artifacts to a reproducible location (artifacts/baseline/) and document command lines/env vars.\n- Ensure uses uv venv, torch compile settings consistent; no code deletions.","acceptance_criteria":"Baseline run completed using uv venv on GPU: record exact commit, config (batch/seq/model, lr, optimizer), FLOPs budget calc, tokens/s, TFLOP/s, memory; artifacts saved under artifacts/baseline/ with JSON+markdown; command lines + env vars documented; seeds set per 8c7; no code deletions.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:06:39.026383362Z","updated_at":"2025-12-18T03:21:23.380218629Z","closed_at":"2025-12-18T03:21:23.380218629Z","close_reason":"Baseline run completed; artifacts saved under artifacts/baseline/ with JSON+markdown and fixed-FLOPs budgeting in nanochat.train","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-5bo","depends_on_id":"model_guided_research-8c7","type":"blocks","created_at":"2025-11-20T08:11:58.192256562Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-5fp","title":"Add validation loss evaluation to nanochat training","description":"Add simple validation support to nanochat.train: (1) --val-fraction flag to hold out X% of data for validation, (2) Periodic validation CE evaluation every N steps, (3) Log both train_ce and val_ce to summary.json. Keep it simple - no separate perplexity/bpb metrics needed initially. This enables fair A/B comparisons by measuring generalization, not just train loss.","status":"in_progress","priority":1,"issue_type":"task","assignee":"","created_at":"2025-12-19T20:55:26.375812798Z","updated_at":"2026-01-23T05:12:17.925043666Z","compaction_level":0}
{"id":"model_guided_research-5td","title":"Document & script GPU env detection/parity","description":"- Add a small doc and optional helper script to capture GPU stack (CUDA version, driver, torch version, jax/CUDA, env flags) to ensure runs are reproducible.\n- Mirror working settings from /tmp/bio_inspired_nanochat; highlight required env vars for FlexAttention/torch.compile.\n- Output stored under docs/gpu_env_notes.md; no code changes required, helper script opt-in.","status":"closed","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:10.869327524Z","updated_at":"2025-12-19T04:17:04.319509462Z","closed_at":"2025-12-19T04:17:04.319509462Z","close_reason":"Added GPU environment capture notes and FlexAttention/torch.compile flags in docs/gpu_env_notes.md","compaction_level":0}
{"id":"model_guided_research-68v","title":"CMA-ES Phase 1 (nanochat bio/exotic params, top-10)","description":"- Define and run a 10D CMA-ES pilot on the most sensitive nanochat/bio-inspired knobs (e.g., tau_rrp, lambda_loge analogue, synaptic gains, structural costs) or closest equivalents in this repo; map to config files; run small-budget evaluations.\n- Use FLOPs-budget harness, seeds, telemetry; store runs in artifacts/cmaes/phase1/ with progress and best params.\n- Report early results and decide Go/No-Go for Phase 2.","acceptance_criteria":"Pilot search spec written; parameter set and bounds finalized; at least one CMA-ES pilot run executed (or dry-run if GPU unavailable) with metrics saved to artifacts/cmaes/phase1/; summary note with best params and Go/No-Go criteria recorded.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:31:58.278455133Z","updated_at":"2025-12-18T06:37:08.307328853Z","closed_at":"2025-12-18T06:37:08.307328853Z","close_reason":"Implemented CMA-ES Phase 1 pilot: added docs/cmaes_phase1.md spec, scripts/cmaes_phase1.py runner, extended nanochat/train.py for GPTSynaptic + --synaptic-config, and executed CPU pilot run with artifacts under artifacts/cmaes/phase1/68v_pilot_cpu (plus smoke).","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-68v","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:31:58.281212835Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-68v","depends_on_id":"model_guided_research-8c7","type":"blocks","created_at":"2025-11-20T08:31:58.284335637Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-68v","depends_on_id":"model_guided_research-ajh","type":"blocks","created_at":"2025-11-20T08:32:39.252963912Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-68v","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:31:58.280005334Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-68v","depends_on_id":"model_guided_research-kt8","type":"blocks","created_at":"2025-11-20T08:31:58.282636836Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-68v","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:31:58.283963637Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-6fl","title":"Tokenizer/dataset pipeline parity vs bio_inspired","description":"- Compare nanochat dataset/dataloader/tokenizer stack with bio_inspired_nanochat (tokenizer.py, dataloader.py, dataset configs).\n- Identify differences in batching, masking, padding, seq length, shuffling, and caching; propose deltas/flags for fair benchmarks.\n- Summarize in docs/data_parity.md with file/line refs; no code changes yet.","acceptance_criteria":"Data/tokenizer parity note compares batching/padding/masking/shuffle/cache defaults with bio_inspired; identifies required changes/flags; includes file/line references; ensures fair benchmark configs.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:18.477784933Z","updated_at":"2025-11-20T08:13:44.983888167Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-6fl","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:09:18.479347241Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-6j9","title":"Benchmark math demos vs vanilla nanochat at fixed FLOPs","description":"- Define evaluation harness to run selected demos (matrix-gauge, reversible, tropical, ultrametric, simplicial, etc.) and vanilla nanochat under matched training/inference FLOPs budgets.\n- Collect metrics: loss, perplexity/accuracy proxies, throughput, memory, stability (NaNs), and math-specific diagnostics; ensure rich tables in output.\n- Automate runs via script/CLI options (no auto-mod scripts) writing JSON+markdown summaries to artifacts/bench/.\n- Provide comparison report highlighting wins/regressions and guidance for future tuning.","acceptance_criteria":"Benchmark harness runs math demos vs vanilla nanochat under matched FLOPs budgets; collects loss/metric, tokens/s, TFLOP/s, memory, stability flags, and math-specific diagnostics; outputs JSON+markdown tables in artifacts/bench/; command lines + seeds recorded; conclusions on win/lose per feature included.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:06:59.309041079Z","updated_at":"2025-12-18T06:56:04.107331529Z","closed_at":"2025-12-18T06:56:04.107331529Z","close_reason":"Implemented fixed-FLOPs nanochat benchmark harness as mgr bench-fixed-flops (cli.py); writes per-run nanochat summaries under artifacts/bench/fixed_flops/nanochat/<suite>/... and suite aggregation summary.json+run.md. Ran CPU smoke suites: artifacts/bench/fixed_flops/nanochat/6j9_smoke_cpu and artifacts/bench/fixed_flops/nanochat/6j9_smoke_certs (includes demo certificate diagnostics).","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-6j9","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:06:59.312579417Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-6j9","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:06:59.311518606Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-6l7","title":"Compare GPU/FlexAttention stack vs bio_inspired_nanochat","description":"- Study /tmp/bio_inspired_nanochat GPU pipeline (configs, torch.compile, env flags, fused kernels, CUDA version expectations).\n- Extract FlexAttention integration pattern (flex_synaptic.py, verify/benchmark scripts) and note prerequisites (torch>=2.5, compile requirements, masking behavior).\n- Document explicit diffs between that repo and current nanochat stack (init, dataloader, optimizer/scheduler, attention paths) with actionable deltas.\n- Summarize in a markdown note (docs/gpu_flex_diff.md) with links to files/lines; no code changes yet.","acceptance_criteria":"Side-by-side diff note produced in docs/gpu_flex_diff.md covering: torch/compile flags, env vars, CUDA versions, attention implementations, optimizer/scheduler, dataloader, activation checkpointing, fused kernels; cites file/line from both repos; actionable deltas listed with owners/next steps; no code changes required.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:06:43.324419367Z","updated_at":"2025-12-18T01:00:35.021239554Z","closed_at":"2025-12-18T01:00:35.021239554Z","close_reason":"Completed docs/gpu_flex_diff.md: added env vars/CUDA/compile/dtype/scheduler/checkpointing/fused-kernel diffs + actionable next steps","compaction_level":0,"comments":[{"id":1,"issue_id":"model_guided_research-6l7","author":"ubuntu","text":"Starting work now: auditing /tmp/bio_inspired_nanochat GPU/FlexAttention stack vs this repo; will produce docs/gpu_flex_diff.md with actionable diffs and prerequisites.","created_at":"2025-12-18T00:40:28Z"}]}
{"id":"model_guided_research-6mr","title":"Port FlexAttention correctness/benchmark scripts","description":"- Adapt bio_inspired_nanochat/scripts/verify_flex_correctness.py and benchmark_flex.py to this repo (respecting AGENTS rules), gated on torch>=2.5.\n- Ensure masking/causal/numerical parity checks; emit rich tables; allow CPU skip.\n- Place under scripts/ with uv-compatible entry instructions; no deletions.","acceptance_criteria":"Scripts ported (verify_flex_correctness, benchmark_flex) runnable via uv; gated on torch>=2.5; produce parity checks + small perf table; rich output; no failures on CPU skip. Stored under scripts/; documented commands and expected outputs.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:18.861605068Z","updated_at":"2025-12-18T02:10:50.949928103Z","closed_at":"2025-12-18T02:10:50.949928103Z","close_reason":"Implemented scripts/verify_flex_correctness.py and scripts/benchmark_flex.py (uv-runnable, rich tables, torch>=2.5 gating) and documented commands in README","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-6mr","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:08:18.863404853Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-6vw","title":"FlexAttention masking/caching edge cases","description":"- Enumerate and test padding/causal/bidirectional/multiquery masking behaviors for FlexAttention path; ensure parity with standard attention and math-specific variants (synaptic, tropical, ultrametric).\n- Define expected shapes/dtypes and dropout handling; capture pitfalls from bio_inspired_nanochat flex_synaptic.\n- Add test plan (not code yet) and note required API surfaces for modules.","acceptance_criteria":"Matrix of masking/caching cases defined (padding, causal, bidir, MQA/MHA, dropout); tests or scripted checks demonstrating parity with standard attention; edge-case notes (shape/dtype constraints) captured; pass/fail recorded in issue comments or artifacts.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:23.195769647Z","updated_at":"2025-12-18T02:13:39.759317508Z","closed_at":"2025-12-18T02:13:39.759317508Z","close_reason":"Defined masking/caching case matrix in docs/flex_attention_edge_cases.md; extended scripts/verify_flex_correctness.py with --suite covering MQA/GQA/MHA + KV-cache (Tq==1 and chunk decode); recorded dtype/compile pitfalls","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-6vw","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:09:23.208009611Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-7dm","title":"Explore derived algebra / higher homotopy learning mechanisms","description":"Long-term research: explore derived algebra / higher homotopy ideas for learning rules or attention. Deliverables: doc + runnable JAX demo + optional nanochat module.","status":"open","priority":4,"issue_type":"epic","assignee":"","created_at":"2025-12-17T22:59:50.256566831Z","updated_at":"2025-12-17T23:00:03.359545176Z","compaction_level":0}
{"id":"model_guided_research-7fi","title":"Convergence curve analysis and extended training runs","description":"Run longer training to analyze convergence behavior: (1) Extended runs to 10x+ baseline FLOPs, (2) Track loss curves over full training, (3) Compare convergence speed (steps to threshold), (4) Identify if exotic attention helps more in early/late training, (5) Plot loss curves with confidence bands. Short runs may miss where benefits appear.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-19T20:56:18.069833908Z","updated_at":"2025-12-19T21:01:12.384060509Z","closed_at":"2025-12-19T21:01:12.384060509Z","close_reason":"Not a separate task - convergence analysis is just running training longer. Can be a parameter to z7r runs (--target-flops) rather than a separate bead.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-7fi","depends_on_id":"model_guided_research-5fp","type":"blocks","created_at":"2025-12-19T20:56:32.932077512Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-7ow","title":"Per-head entropy/diversity visualization","description":"- Add visualization/report of per-head attention entropy and route diversity (for tropical/ultrametric/synaptic heads) comparing vanilla vs math features.\n- Run on small batches with seeds; export tables and plots to artifacts/vis/entropy/.\n- Integrate with per-head error-bars task if possible.","acceptance_criteria":"Entropy/diversity metrics computed per head for baseline + ≥1 feature config; plots/tables saved to artifacts/vis/entropy/ with commands; seeds recorded; interpretation notes included.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:20:01.894747140Z","updated_at":"2025-11-20T08:20:01.894747140Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-7ow","depends_on_id":"model_guided_research-pv0","type":"blocks","created_at":"2025-11-20T08:20:06.055875302Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-7sl","title":"Statistical significance framework for benchmark results","description":"Add statistical analysis to benchmark comparisons: (1) Welch's t-test or Mann-Whitney for pairwise comparisons, (2) Holm-Bonferroni correction for multiple comparisons, (3) 95% CI calculation and display, (4) Effect size (Cohen's d), (5) Rich table output with significance stars. Essential for scientific rigor in claims.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-12-19T20:55:58.870691859Z","updated_at":"2025-12-19T21:01:13.778237286Z","closed_at":"2025-12-19T21:01:13.778237286Z","close_reason":"Overkill - basic statistics (mean, std, CI, t-test) can be computed inline during z7r implementation. A separate 'framework' is over-engineering.","compaction_level":0}
{"id":"model_guided_research-7ut","title":"Scaling study: model size and sequence length experiments","description":"Test how exotic attention benefits scale: (1) Model sizes: tiny (4L/128d), small (6L/256d), medium (12L/512d), (2) Sequence lengths: 256, 512, 1024, 2048, (3) Track if relative advantage increases/decreases with scale, (4) Memory/throughput vs accuracy trade-offs, (5) Document expected vs actual scaling behavior. Critical for understanding when these ideas matter.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-19T20:56:00.392162660Z","updated_at":"2025-12-19T21:01:11.382483165Z","closed_at":"2025-12-19T21:01:11.382483165Z","close_reason":"Premature - scaling studies should wait until basic A/B comparison (z7r) is working. Can revisit after z7r is complete.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-7ut","depends_on_id":"model_guided_research-e8k","type":"blocks","created_at":"2025-12-19T20:56:31.718470407Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-827","title":"CA structure retention experiment","description":"- Test whether freezing a fraction of CA-initialized channels/heads for N warmup steps or adding CA as a low-rank bias improves retention/perf.\n- Run small ablation (freeze vs no-freeze; bias vs pure) on one config; measure early loss + any stability changes.\n- Document findings; do not change defaults.","acceptance_criteria":"One ablation run comparing freeze/bias variants vs plain CA init; metrics recorded; short write-up added to artifacts/ca_init/retention.md; no default behavior changes.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:26:03.060416992Z","updated_at":"2025-11-20T08:26:03.060416992Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-827","depends_on_id":"model_guided_research-x1g","type":"blocks","created_at":"2025-11-20T08:26:19.115623395Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-83g","title":"Synaptic: reconcile PostsynapticHebb consolidation spec; remove silent no-ops","description":"nanochat/synaptic.py contains explicit dimensionality ambiguity in PostsynapticHebb consolidation (e.g. diag/mean fallbacks) and returns early on shape mismatch. Reconcile intended math/spec with implementation (and GPTSynaptic usage), eliminate silent no-op updates, and add targeted unit tests for shape/gradient safety.","acceptance_criteria":"PostsynapticHebb consolidation/update paths are dimensionally well-defined for all callsites (SynapticLinear + attention paths); no silent early-return no-ops; tests cover at least one consolidation step and assert state changes + no NaNs.","status":"closed","priority":2,"issue_type":"bug","assignee":"","created_at":"2025-12-18T08:20:04.734096016Z","updated_at":"2025-12-18T08:39:05.890516576Z","closed_at":"2025-12-18T08:39:05.890516576Z","close_reason":"Implemented well-defined PostsynapticHebb delta (mean over presyn dim), removed silent no-ops (raise on mismatch), added regression tests; pytest+ubs clean.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-83g","depends_on_id":"model_guided_research-3bx","type":"discovered-from","created_at":"2025-12-18T08:20:04.739334514Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-8c7","title":"Reproducibility & seed discipline","description":"- Specify standard seeding pattern across JAX, NumPy, torch, random, and dataloaders; ensure CLI/tests accept seed flag and propagate.\n- Draft checklist and hooks to detect missing seeding in new scripts.\n- Provide guidance in docs/repro.md; minimal code tweaks allowed only if non-invasive (adding seed plumbed args).","acceptance_criteria":"Single seeding guideline covering torch, numpy, random, JAX, dataloaders; CLI/test flags accept seed and propagate; doc snippet in docs/repro.md; quick checklist to verify seeding in new scripts.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:26.353843502Z","updated_at":"2025-12-18T02:54:17.054740278Z","closed_at":"2025-12-18T02:54:17.054740278Z","close_reason":"Added docs/repro.md; plumbed --seed through mgr run-all/eval + nanochat.train; standardized RNG seeding in nanochat.common.compute_init","compaction_level":0}
{"id":"model_guided_research-92m","title":"Per-head route diversity heatmaps in dashboard","description":"- Extend training dashboard to include per-head route diversity/entropy heatmaps (e.g., for tropical/ultrametric/synaptic heads) inline in terminal and HTML modes.\n- Pull data from per-head metrics task; keep overhead minimal and toggleable.","acceptance_criteria":"Heatmaps added to dashboard (terminal + HTML) fed by per-head metrics; toggleable; tested on small run; saved in artifacts/dashboard/ with seeds/commands.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:29:46.724756884Z","updated_at":"2025-11-20T08:29:46.724756884Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-92m","depends_on_id":"model_guided_research-nyp","type":"blocks","created_at":"2025-11-20T08:29:49.748180046Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-92m","depends_on_id":"model_guided_research-pv0","type":"blocks","created_at":"2025-11-20T08:29:52.860642945Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-9f1","title":"Pedagogical walkthrough mode","description":"- Add a guided run option that prints stepwise explanations of the math during a demo/training mini-run (e.g., what Cayley step does, why ultrametric routing sub-quadratic, what tropical margin means), with links to markdown docs.\n- Keep verbose but structured; toggle via flag; ensure does not slow standard runs when off.","acceptance_criteria":"Guided walkthrough mode for at least one demo + one nanochat mini-run; prints stepwise math explanations tied to equations/variables; links to markdown docs; off by default; negligible overhead when disabled.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:15:50.179583749Z","updated_at":"2025-11-20T08:16:46.116769670Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-9f1","depends_on_id":"model_guided_research-3fi","type":"blocks","created_at":"2025-11-20T08:16:09.345088972Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-a1o","title":"Ultrametric: implement cache-opt packed trie layout (bitsets + rank/select)","description":"Implement the production layout sketched in markdown_documentation/ultrametric_worlds_and_p_adic_computation.md: per-depth contiguous arrays + occupancy bitsets with rank/select mapping so lookup/update touch O(K) cache lines with no Python dict/list pointer chasing. Keep the current reference trie implementation for clarity, but add an opt-in packed path (or a separate mode) that matches the doc’s Lookup/Update pseudocode. Add a small correctness + speed sanity check comparing packed vs reference on Tasks A/B.","acceptance_criteria":"Packed layout exists and is selectable; lookup/update results match reference on Tasks A/B; basic perf sanity check demonstrates reduced Python overhead / more contiguous memory access; docs note supported shapes/limits.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-18T07:41:59.961612030Z","updated_at":"2025-12-18T10:09:44.292786510Z","closed_at":"2025-12-18T10:09:44.292786510Z","close_reason":"Implemented packed trie (mask+child table) option; added packed/reference parity+timing checks for Tasks A/B; fixed Task B leaf-exceptions dataset bug; tests passing.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-a1o","depends_on_id":"model_guided_research-3fi","type":"discovered-from","created_at":"2025-12-18T07:41:59.987715904Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-a3u","title":"CMA-ES resume/preemption robustness","description":"- Ensure resumed runs after interruption (spot/preempt) work: checkpoint integrity check, partial-generation recovery, and optional s3/rsync sync guidance.\n- Add a short resume how-to and verification step.","acceptance_criteria":"Resume path documented and tested (checkpoint reload + continue generation); integrity check included; optional sync guidance added; sample resume log saved to artifacts/cmaes/.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:34:20.049531995Z","updated_at":"2025-11-20T08:34:20.049531995Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-a3u","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:34:20.064974086Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-ac9","title":"Auto regression gate","description":"- Add an optional gate that reads regression guardrail results and exits nonzero (or posts a clear warning) when regressions exceed thresholds; configurable per-metric.\n- Integrate with harness/report pipeline; allow bypass via flag/env for exploratory runs.\n- Document workflow and defaults.","acceptance_criteria":"Gate implemented and toggleable; triggers on guardrail outputs with configurable thresholds; integrated into harness/report flow; sample run showing nonzero exit and warning message; docs updated.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:29:35.439654542Z","updated_at":"2025-11-20T08:29:35.439654542Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-ac9","depends_on_id":"model_guided_research-eg9","type":"blocks","created_at":"2025-11-20T08:29:40.321995424Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-ajh","title":"CMA-ES infra for MGR (objective + distribution)","description":"- Scope CMA-ES integration for this repo: define objective wrapper(s) for nanochat training and/or math-demo runs with fixed FLOPs budget; plan distributed evaluation and checkpointing layout; map to artifacts dirs and telemetry schema.\n- Produce design doc (docs/cmaes_plan_mgr.md) outlining parameter vector encoding, bounds (log vs linear), seed handling, resume/checkpoint strategy, and integration points with harness.\n- No code changes yet.","acceptance_criteria":"Design doc committed under docs/cmaes_plan_mgr.md covering objective API, parameter encoding, bounds, seeds, FLOPs budget use, checkpoint/resume, and distributed eval approach; references artifacts layout and telemetry schema; ready for implementation with minimal ambiguity.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:31:52.163710056Z","updated_at":"2025-12-18T02:26:58.281914915Z","closed_at":"2025-12-18T02:26:58.281914915Z","close_reason":"Design doc written at docs/cmaes_plan_mgr.md","compaction_level":0}
{"id":"model_guided_research-b1l","title":"GPU profiling & NVTX/torch.profiler instrumentation","description":"- Add optional profiling hooks (NVTX ranges / torch.profiler / JAX prof) around attention/optimizer hot paths to measure latency, kernel time, memory.\n- Provide CLI flags/env vars to enable; default off; ensure rich summary tables.\n- Document workflow for capturing traces and comparing Flex vs standard attention.","acceptance_criteria":"Profiling hooks added and documented: NVTX/torch.profiler toggles via flags/env; sample trace captured (Flex on/off) showing kernel/memory breakdown; how-to doc with commands; overhead when disabled is negligible.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:23.695573748Z","updated_at":"2025-11-20T08:13:02.888182378Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-b1l","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:08:23.707289727Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-b5l","title":"FlexAttention compatibility matrix (versions/hardware)","description":"- Build a matrix of torch versions, CUDA drivers, GPUs, and whether flex works (compile success, runtime stability, perf). Use bio_inspired_nanochat experience as starting row.\n- Document required flags (torch.compile mode, enable/disable cudagraphs) and fallbacks.\n- Publish as docs/flex_compat_matrix.md; no code changes.","acceptance_criteria":"FlexAttention compat matrix populated with at least one working and one failing combo; includes torch/CUDA versions, GPU models, driver, compile flags, outcomes; mitigation notes for failures; stored at docs/flex_compat_matrix.md.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:21.726310693Z","updated_at":"2025-11-20T08:13:38.268746856Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-b5l","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:10:21.732819894Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-bhv","title":"Richer test coverage for math-specific invariants","description":"- Extend tests/test_mathematical_properties.py (or new cases) to assert properties advertised in docs: orthogonality errors, symplectic checks, tropical margin certificates, ultrametric scaling, braid invariance, etc.\n- Keep tests seedable and fast; gate GPU-only pieces; use rich output.\n- No deletion of existing tests; add targeted cases with clear comments.","acceptance_criteria":"New/expanded tests cover claimed math invariants (orthogonality, symplectic, margin certs, scaling) with seeds; run fast locally/CI; rich output; no flakiness.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:38.082423366Z","updated_at":"2025-12-19T04:15:55.894171670Z","closed_at":"2025-12-19T04:15:55.894171670Z","close_reason":"Added math invariant tests: Cayley orthogonality, symplectic check, tropical margin cert, ultrametric inequality, braid YBE","compaction_level":0}
{"id":"model_guided_research-bks","title":"FLOPs vs measured TFLOPs validation","description":"- Cross-validate theoretical FLOPs calculations against profiler-measured TFLOP/s for baseline and math-feature runs (including FlexAttention path).\n- Identify any discrepancies; adjust accounting or note overhead (padding, compile, data movement).\n- Provide a short guide on how to measure accurately (profiler instructions, warmup rules).","acceptance_criteria":"Side-by-side table of computed FLOPs vs measured TFLOP/s for ≥2 configs (baseline + one feature + Flex); discrepancy analysis included; guidance doc added to docs/flops_validation.md.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:14:48.321547129Z","updated_at":"2025-11-20T08:14:48.321547129Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-bks","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:15:10.505000250Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-bks","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:15:07.945957641Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-blm","title":"JAX pathway audit for GPU readiness","description":"- Verify JAX/Flax demos run on GPU with current deps (jax[cuda], cuda libs); check config.setup_jax and env defaults.\n- Add guidance for enabling GPU path (XLA flags, memory preallocate) while keeping CPU-safe defaults.\n- Record findings and any required package/version adjustments; do not remove CPU fallbacks.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:33.625353169Z","updated_at":"2025-11-20T08:08:33.625353169Z","compaction_level":0}
{"id":"model_guided_research-bwz","title":"Regression dashboard (historical benchmark diffs)","description":"- Build a small tool/report that loads historical benchmark JSONs (baseline, Flex, feature runs) and shows regressions/improvements with thresholds; include sparklines and color coding.\n- Integrate with artifacts layout and telemetry schema; produce markdown + optional HTML.\n- Detect significant deltas in loss, tokens/s, TFLOP/s, memory, key diagnostics; flag when outside band.","acceptance_criteria":"Dashboard/report can diff at least two benchmark snapshots and highlight regressions/improvements; uses artifacts layout; markdown (and optional HTML) output generated with commands; thresholds configurable; sample run saved in artifacts/regressions/.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:18:40.546638600Z","updated_at":"2025-12-18T10:48:08.449863795Z","closed_at":"2025-12-18T10:48:08.449863795Z","close_reason":"Implemented mgr regressions dashboard with markdown+HTML reports, configurable thresholds, and sample artifacts; added CLI smoke test.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-bwz","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:19:06.738046430Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-bwz","depends_on_id":"model_guided_research-kt8","type":"blocks","created_at":"2025-11-20T08:19:01.695852856Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-bwz","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:18:58.999975670Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-bya","title":"Explore category theory (functorial networks) integration","description":"Long-term research: prototype a category-theory-inspired component (e.g., functorial layers / compositional wiring). Deliverables: markdown doc in markdown_documentation/, a runnable JAX demo, and a nanochat attention/block variant behind a flag.","status":"open","priority":4,"issue_type":"epic","assignee":"","created_at":"2025-12-17T22:59:50.126044469Z","updated_at":"2025-12-17T23:00:03.325654820Z","compaction_level":0}
{"id":"model_guided_research-c6h","title":"Triton hotspot benchmarking plan","description":"- Identify attention/MLP/kernels that would benefit most from Triton (see bio_inspired_nanochat PLAN_TO_OPTIMIZE_KEY_HOTSPOTS_USING_TRITON.md for cues).\n- Prototype microbench checklist (not code yet) and pick 1-2 candidate ops in nanochat to target after Flex path lands.\n- Deliver markdown note with ops, expected speedup rationale, and prerequisite shapes/constraints; no code changes now.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:11.260496601Z","updated_at":"2025-11-20T08:09:11.260496601Z","compaction_level":0}
{"id":"model_guided_research-c6v","title":"CI-lite uv workflow (local script)","description":"- Create a local script to run lint/tests with uv (ruff, pytest selected files) mirroring AGENTS constraints; no GitHub Actions required.\n- Ensure respects venv, no pip; add instructions in docs/ci_lite.md.\n- Goal: quick pre-push check covering changed files and FlexAttention skips when torch<2.5.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:39.548766221Z","updated_at":"2025-11-20T08:09:39.548766221Z","compaction_level":0}
{"id":"model_guided_research-clq","title":"Nanochat vs demos integration points","description":"- Map which math demos (gauge, reversible, tropical, ultrametric, simplicial, etc.) can be plugged into nanochat training stack; list API gaps, tensor shape expectations, and config knobs needed for drop-in.\n- Propose interface shims (non-breaking) and flag required work; document in docs/integration_matrix.md.","acceptance_criteria":"Integration matrix lists which demos can plug into nanochat, APIs needed, tensor shapes, config knobs; highlights blockers; actionable follow-ups enumerated.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:33.122617718Z","updated_at":"2025-11-20T08:13:48.296708637Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-clq","depends_on_id":"model_guided_research-3bx","type":"blocks","created_at":"2025-11-20T08:10:33.127245620Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-e8k","title":"Systematic attention-type A/B benchmark suite","description":"Comprehensive A/B benchmark comparing all 11 exotic attention types vs vanilla baseline: (1) mgr bench-attention-suite command, (2) Fixed FLOPs budget with same architecture, (3) Multiple seeds per config for statistical power, (4) Collect train/val CE, convergence speed, throughput, memory, (5) Output comparison tables with significance tests. KEY deliverable for proving real-world benefit.","status":"closed","priority":1,"issue_type":"feature","assignee":"","created_at":"2025-12-19T20:55:26.646104195Z","updated_at":"2025-12-19T21:01:06.246467048Z","closed_at":"2025-12-19T21:01:06.246467048Z","close_reason":"Redundant with existing z7r (Granular A/B feature benchmarking) which covers the same functionality. z7r's deps are all closed and it's ready to implement.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-e8k","depends_on_id":"model_guided_research-5fp","type":"blocks","created_at":"2025-12-19T20:56:29.636892819Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-e8k","depends_on_id":"model_guided_research-7sl","type":"blocks","created_at":"2025-12-19T20:56:30.817590636Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-eg9","title":"Perf regression guardrails","description":"- Define thresholds for key metrics (tokens/s, TFLOP/s, loss delta, memory) and a small script to compare latest run vs reference artifacts and flag regressions.\n- Integrate with regression dashboard outputs; optional non-blocking exit codes.\n- Document how to set/update baselines and exceptions.","acceptance_criteria":"Script/report that compares a new run against a chosen baseline and flags regressions beyond configurable thresholds; supports at least tokens/s, TFLOP/s, loss, memory; docs on updating baselines; sample output saved under artifacts/regressions/.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:19:54.165829497Z","updated_at":"2025-12-18T10:52:15.755166153Z","closed_at":"2025-12-18T10:52:15.755166153Z","close_reason":"Added guardrail exit codes to mgr regressions (--fail-on-regression/--fail-on-missing) and documented baseline/threshold workflow in artifacts/README.md.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-eg9","depends_on_id":"model_guided_research-bwz","type":"blocks","created_at":"2025-11-20T08:19:57.765932426Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-gig","title":"Nanochat tropical: add gauge-fixing + margin certificates/diagnostics","description":"Implement the tropical gauge-fixing + certificate pieces described in markdown_documentation/tropical_geometry_and_idempotent_algebra.md: score-centering/anchoring invariances and per-layer runner-up margins (gamma) usable as robustness/uncertainty certs. Add optional logging hooks so training can track tropical margins per head (feeds dashboard work).","acceptance_criteria":"Tropical attention reports per-head (or per-token) margin/cert values; gauge-fixing mode implemented and documented; unit test(s) assert cert/margin behavior is stable under allowed gauge shifts; no regressions in existing tests.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-18T08:19:58.229885118Z","updated_at":"2025-12-18T08:54:51.090629320Z","closed_at":"2025-12-18T08:54:51.090629320Z","close_reason":"Added tropical gauge-fixing + score-centering + runner-up margin (gamma) reporting in TropicalCausalSelfAttention; wired train.py CLI/logging; documented flags; added gauge-invariance test; pytest+ubs clean.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-gig","depends_on_id":"model_guided_research-3bx","type":"discovered-from","created_at":"2025-12-18T08:19:58.256653826Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-gjm","title":"Design FLOPs-budgeted benchmark harness","description":"- Specify FLOPs accounting method (tokens * model FLOPs/token) for nanochat and each math demo; document assumptions.\n- Add harness entrypoints/CLI flags to run workloads at fixed FLOPs, exporting metrics JSON (loss, tokens/s, TFLOP/s, memory) and rich table output.\n- Ensure repeatable seeds/configs and minimal boilerplate; no auto code-mod scripts.","acceptance_criteria":"Harness design doc and initial implementation stubs that: (a) define FLOPs calc per run, (b) take config/flags for model features, (c) emit structured JSON + rich tables, (d) integrate artifacts layout + telemetry schema, (e) support seed propagation. At least one dry-run producing sample output.","status":"closed","priority":1,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:15.413564977Z","updated_at":"2025-12-18T06:00:26.893640322Z","closed_at":"2025-12-18T06:00:26.893640322Z","close_reason":"Completed: documented FLOPs accounting + usage in docs/fixed_flops_harness.md; nanochat fixed-FLOPs runner now supports artifacts kind/topic and emits mgr.telemetry.v1 (meta/budget/results) with rich summary; mgr run exposes --max-iterations for demo compute caps; dry-run saved under artifacts/bench/fixed_flops/nanochat/.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-gjm","depends_on_id":"model_guided_research-kt8","type":"blocks","created_at":"2025-11-20T08:11:50.450067623Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-gjm","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:11:46.868826876Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-hi3","title":"Insightful model state visualizations","description":"- Plan and implement lightweight visualizations of internal states: attention patterns (flex/standard), gauge angles/curvature, reversible det checks, tropical route margins, ultrametric occupancy histograms, simplicial Hodge spectra.\n- Provide saving to HTML/PNG (matplotlib or rich plots) with small sample batches; ensure reproducible seeds; integrate with artifacts layout.","acceptance_criteria":"≥3 visualizations (attention maps, gauge angles/curvature trend, reversible det/symplectic err, tropical route margins, ultrametric occupancy, or simplicial Hodge spectrum) runnable on sample batches; saved to artifacts/vis/ with commands; seeded; doc on interpretation.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:15:46.381912495Z","updated_at":"2025-11-20T08:16:42.869251473Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-hi3","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:16:04.584388548Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-hi3","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:16:02.088415694Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-iso","title":"Rich console styling audit","description":"- Review current Rich output across CLI, demos, tests to ensure informative/stylish per AGENTS.md; list places that still print plain text or lack color/structure.\n- Propose improvements and consistent themes; no code changes yet.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:53.699721312Z","updated_at":"2025-11-20T08:10:53.699721312Z","compaction_level":0}
{"id":"model_guided_research-k2y","title":"Braid attention: add YBE-satisfying crossing law option","description":"The braid doc formalism assumes a crossing map satisfying the set-theoretic Yang–Baxter equation (3-strand coherence) to make R3 sound. The current demo intentionally restricts to σ₁^k and uses a crossing_update that is invertible but not YBE. Add an optional crossing law (and minimal tests) that *does* satisfy YBE, so the demo can legitimately support broader braid words and R3 reductions when enabled. Keep the existing crossing_update as the fast restricted mode.","acceptance_criteria":"New crossing law option included with proof-by-test that YBE holds on random samples; demo can run in both modes (restricted vs YBE); docs/report clarify which algebraic guarantees apply to each mode.","status":"closed","priority":3,"issue_type":"task","assignee":"","created_at":"2025-12-18T07:42:09.486546111Z","updated_at":"2025-12-18T11:07:07.696534887Z","closed_at":"2025-12-18T11:07:07.696534887Z","close_reason":"Added optional YBE-valid crossing law (crossing_update_ybe) with decoder support and unit test proving R3/YBE on random samples; updated braid markdown docs to clarify restricted vs YBE guarantees and how to enable via BRAID_CROSSING_LAW.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-k2y","depends_on_id":"model_guided_research-3fi","type":"discovered-from","created_at":"2025-12-18T07:42:09.491274718Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-kt8","title":"Telemetry/logging standardization","description":"- Propose a minimal logging schema (JSON + rich) for training/bench runs: loss, tokens/s, lr, grad norm, memory, TFLOP/s, feature flags (flex, math modules), seed, FLOPs budget.\n- Ensure fields align with artifacts layout; recommend using structlog or simple dict emission.\n- Doc-first; code changes later.","acceptance_criteria":"Telemetry schema agreed and documented (fields, units, required/optional); sample JSON and rich table format provided; adopted by harness spec; aligns with artifacts layout.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:36.959111123Z","updated_at":"2025-12-18T05:49:45.556623544Z","closed_at":"2025-12-18T05:49:45.556623544Z","close_reason":"Completed: documented minimal telemetry schema (mgr.telemetry.v1) + sample JSON + recommended rich summary table in artifacts/README.md; referenced schema in docs/cmaes_plan_mgr.md for harness/CMA-ES adoption.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-kt8","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:10:36.983479651Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-lbp","title":"Fail-fast sanity checks for configs","description":"- Add a checklist/design for config validation: incompatible flag combos (flex without torch>=2.5, ultrametric packed on CPU?), missing env vars, mismatched batch/seq with model dims.\n- Plan for lightweight runtime asserts with rich error messages; code changes later after design approval.","acceptance_criteria":"List of invalid/incompatible config combos defined; plan for runtime validation (rich errors) documented; at least one prototype check implemented or scoped with exact locations; no spurious blocks on valid configs.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:50.679175347Z","updated_at":"2025-12-18T09:23:59.934381265Z","closed_at":"2025-12-18T09:23:59.934381265Z","close_reason":"Added fail-fast config validation in nanochat/train.py (argparse choices + _validate_train_args) and documented invalid combos/plan in docs/repro.md.","compaction_level":0}
{"id":"model_guided_research-lpg","title":"UBS adoption for changed files","description":"- Install Ultimate Bug Scanner locally (per AGENTS.md) and define workflow to run \"ubs <changed-files>\" before commits.\n- Add brief usage doc under docs/ubs_usage.md; no enforcement hooks.\n- Optionally add helper alias/script for per-file runs (manual invocation).","status":"open","priority":3,"issue_type":"chore","assignee":"","created_at":"2025-11-20T08:09:35.268279362Z","updated_at":"2025-11-20T08:09:35.268279362Z","compaction_level":0}
{"id":"model_guided_research-m32","title":"CA init early-phase benchmark","description":"- Add harness cases to compare CA init vs standard (and mix) on a small model/config (same FLOPs budget, seeds) for first 500-2000 steps.\n- Record early loss, grad norms, activation stats, stability flags; store results in artifacts/ca_init/ with JSON+markdown.\n- Include at least one attention-heavy and one MLP-heavy setting.","acceptance_criteria":"Benchmark run executed for baseline vs CA (and mix) with ≤2 configs; metrics captured (loss curve, grad norm, act stats) and saved under artifacts/ca_init/; seeds/commands recorded.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:25:55.995359332Z","updated_at":"2025-11-20T08:25:55.995359332Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-m32","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:26:15.953027691Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-m32","depends_on_id":"model_guided_research-x1g","type":"blocks","created_at":"2025-11-20T08:26:10.924409659Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-m72","title":"Synaptic complexin clamp gate uses clamp state","description":"Synaptic presyn release probability currently ignores the complexin clamp state (`cl`) even though it is tracked/updated and advertised in comments/docs; FlexAttention precompute path also ignores clamp. This makes `cl` a dead state and diverges from fused-kernel and bio_inspired formula.","acceptance_criteria":"`cl` influences mix probability in both manual and FlexAttention synaptic paths (and supports optional `complexin_bias`); tests pass; no new NaN/Inf regressions.","status":"closed","priority":2,"issue_type":"bug","assignee":"","created_at":"2025-12-18T02:34:15.320408540Z","updated_at":"2025-12-18T02:36:22.701592922Z","closed_at":"2025-12-18T02:36:22.701592922Z","close_reason":"Implemented clamp-aware mix_prob + added complexin_bias; synced FlexAttention and manual paths; tests pass","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-m72","depends_on_id":"model_guided_research-2e5","type":"discovered-from","created_at":"2025-12-18T02:34:15.343861216Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-my1","title":"Explore topos theory / higher categorical attention ideas","description":"Long-term research: explore topos-theory / higher categorical structures as attention/representation mechanisms. Deliverables: doc + runnable JAX demo + optional nanochat module.","status":"open","priority":4,"issue_type":"epic","assignee":"","created_at":"2025-12-17T22:59:50.220800634Z","updated_at":"2025-12-17T23:00:03.391308666Z","compaction_level":0}
{"id":"model_guided_research-nyp","title":"Rich training dashboard + live insights","description":"- Design a rich console/HTML dashboard showing: loss/throughput/TFLOP/s, memory, LR, grad norm, NaN flags, feature flags (flex, math modules), and key math diagnostics (e.g., orthogonality error, tropical margin, ultrametric occupancy) updated during runs.\n- Include small ASCII sparklines/heatmaps for quick terminal use; optional HTML export for deeper drill-down.\n- Wire into telemetry schema and artifacts layout; keep overhead minimal when disabled.","acceptance_criteria":"Dashboard design + prototype running on short training loop; shows loss/throughput/TFLOP/s, memory, LR, grad norm, NaN flags, feature flags, plus ≥2 math diagnostics (e.g., orthogonality error, tropical margin); terminal + HTML mode; toggleable; outputs saved to artifacts/dashboard/.","status":"open","priority":1,"issue_type":"feature","assignee":"","created_at":"2025-11-20T08:15:41.045748377Z","updated_at":"2025-11-20T08:16:39.294664078Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-nyp","depends_on_id":"model_guided_research-kt8","type":"blocks","created_at":"2025-11-20T08:15:55.778787595Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-nyp","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:15:58.248581049Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-pf7","title":"CA init safety & scaling checks","description":"- Add sanity tests: variance matches target, no pathological skew/kurtosis, compatible with fan-in/out across shapes; ensure integrable with mixed precision and FlexAttention.\n- Include a small unit test or script to validate stats on representative tensor shapes.\n- Fail fast if flags set without torch>= needed version or shapes unsupported.","acceptance_criteria":"Sanity checks scripted and run on representative shapes; variance/mean within tolerance; documentation of supported shapes/dtypes; guardrails for unsupported cases; results noted in issue or artifacts/ca_init/.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:26:07.906332066Z","updated_at":"2025-12-18T07:28:53.094709908Z","closed_at":"2025-12-18T07:28:53.094709908Z","close_reason":"Added CA init guardrails + broader stats tests (shapes/rules/dtypes) and documented supported weights/dtypes.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-pf7","depends_on_id":"model_guided_research-x1g","type":"blocks","created_at":"2025-11-20T08:26:23.558781010Z","created_by":"daemon","metadata":"{}","thread_id":""}],"comments":[{"id":2,"issue_id":"model_guided_research-pf7","author":"ubuntu","text":"Added safety/scaling checks: CA init now mixes in fp32 for bf16/fp16 weights (then casts), and raises with module/shape context if CA generation fails. Added broader pytest coverage (rule30+rule116; shapes 64x64, 127x33, 33x127, 8x512) verifying mean≈0, std within 0.5% of target, non-pathological skew/kurtosis, plus bf16 mix smoke. Verified via: uv run pytest tests/test_cmaes_objective.py -q","created_at":"2025-12-18T07:28:48Z"}]}
{"id":"model_guided_research-pv0","title":"Per-head/kernel error bars & stability metrics","description":"- Add measurement routines to compute per-head (and per-kernel if available) variability/error bars across seeds for key metrics (loss contrib, attention entropy, margin, orthogonality error, occupancy).\n- Run on small representative configs (baseline, Flex, one math feature) with ≥3 seeds.\n- Export per-head tables with mean/std/CI to artifacts/feature_ablate/ or artifacts/microbench/; integrate with harness/telemetry schema.","acceptance_criteria":"Per-head (and kernel where applicable) metrics collected for ≥3 seeds on baseline + ≥1 feature; mean/std/CI reported; saved to artifacts (JSON+markdown) with commands/seeds; overhead acceptable for small configs.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:18:35.084513282Z","updated_at":"2025-12-18T11:45:25.698764345Z","closed_at":"2025-12-18T11:45:25.698764345Z","close_reason":"Implemented per-head entropy/margins collection + multi-seed suite with mean/std/CI artifacts.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-pv0","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:18:54.940939251Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-pv0","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:18:52.087632021Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-pv0","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:18:49.240140497Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-q8f","title":"CMA-ES multi-seed / split-pop runs","description":"- Plan and optionally run multiple CMA-ES trials with different seeds or split populations to estimate variance and avoid seed luck.\n- Define aggregation of best params (mean/consensus) and reporting of std.\n- Document recommended number of trials vs budget.","acceptance_criteria":"Multi-seed plan documented; at least a pilot with ≥2 seeds or simulated split-pop run executed or scoped; variance reporting method defined; guidance captured in cmaes_plan_mgr or addendum.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:34:13.536998990Z","updated_at":"2025-11-20T08:34:13.536998990Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-q8f","depends_on_id":"model_guided_research-68v","type":"blocks","created_at":"2025-11-20T08:34:13.551437279Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-qha","title":"Cross-repo change watch & sync cadence","description":"- Set up a lightweight process to reread bio_inspired_nanochat periodically for improvements (Flex, kernels, optimizers), summarizing deltas and deciding on ports.\n- Could be a manual checklist or script that diffs key directories and emits a summary (no auto code mods).\n- Document cadence, responsible person, and how to record decisions (issue comments/log).","acceptance_criteria":"Cadence/process doc created (docs/cross_repo_sync.md) describing what to watch (files/dirs), how often, how to summarize, and how to decide on ports; optionally a simple diff script/command listed; first sync summary logged.","status":"open","priority":3,"issue_type":"chore","assignee":"","created_at":"2025-11-20T08:17:45.858238390Z","updated_at":"2025-11-20T08:17:45.858238390Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-qha","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:17:58.085981520Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-qq7","title":"Meta-eval: math-feature ablations","description":"- Plan an ablation matrix turning math features on/off (e.g., gauge structured blocks, ultrametric packed arrays, tropical sparse modes, reversible Cayley variants) under fixed FLOPs to isolate their contributions.\n- Define metrics and logging fields; integrate with benchmark harness design.\n- Output a markdown plan; code changes later.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:09:52.991775890Z","updated_at":"2025-11-20T08:09:52.991775890Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-qq7","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:09:52.993670395Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-s53","title":"Shared config parity tests (bio_inspired ↔ MGR)","description":"- Define a small battery of parity tests/configs runnable in both repos (or replicated here) to compare baseline/Flex/feature toggles under matched hyperparameters.\n- Capture command lines, seeds, and expected metrics to detect drift over time.\n- Store configs/commands in docs/config_parity_suite.md; align with FLOPs harness and baseline tasks.","acceptance_criteria":"Config parity suite defined with ≥3 matched runs (baseline, Flex, one math feature) including commands, seeds, expected metric bands; runnable here (and/or mirrored instructions for bio_inspired); documented in docs/config_parity_suite.md.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:17:39.409394258Z","updated_at":"2025-11-20T08:17:39.409394258Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-s53","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:17:53.909966944Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-s53","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:17:51.152619650Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-s62","title":"Investigate torch.compile failure for synaptic FlexAttention KV-cache paths","description":"On torch 2.9.1+cu128, torch.compile of synaptic FlexAttention fails in KV-cache paths (verify_flex_correctness --model synaptic --compile), with Inductor/SymInt errors (e.g., AttributeError: 'int' object has no attribute 'is_Add' while lowering torch.sym_max) and earlier FlexibleLayout indexing failures. Investigate minimal repro and implement a workaround or document a compatibility matrix.","acceptance_criteria":"Repro captured (exact torch/cuda/flags) and either: (a) synaptic FlexAttention compiles for full_forward + KV-cache paths, or (b) clearly documented as unsupported with version constraints + skip logic in scripts/tests.","status":"closed","priority":2,"issue_type":"bug","assignee":"","created_at":"2025-12-18T04:26:04.419318979Z","updated_at":"2025-12-18T04:57:37.055687490Z","closed_at":"2025-12-18T04:57:37.055687490Z","close_reason":"Fix: use torch.compile(dynamic=False) for synaptic FlexAttention checks to avoid Inductor SymInt lowering failure in KV-cache paths; verified verify_flex_correctness --model synaptic --compile now passes.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-s62","depends_on_id":"model_guided_research-0jk","type":"discovered-from","created_at":"2025-12-18T04:26:04.420691863Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-u8l","title":"HP search plan (CMA-ES / grid)","description":"- Draft a lightweight plan for hyperparameter sweeps under fixed FLOPs using CMA-ES or grid (see bio_inspired_nanochat plan doc). Identify search spaces for Flex vs standard, math features.\n- Specify budget, metrics, and how to capture results to artifacts.\n- No code yet; document in docs/hparam_search_plan.md.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:10:40.203944745Z","updated_at":"2025-11-20T08:10:40.203944745Z","compaction_level":0}
{"id":"model_guided_research-uny","title":"Unified artifacts logging layout","description":"- Define a consistent artifacts directory structure (benchmarks, correctness, profiles, certs) and ensure CLI/scripts write JSON+markdown summaries accordingly.\n- Update existing scripts to honor the layout without breaking current paths; add README describing conventions.\n- Include pointer to FLOPs-budget harness outputs and flex/standard comparisons.","acceptance_criteria":"Artifacts directory layout defined and documented; existing/pending scripts updated to target it or migration plan written; includes subdirs for baseline, bench, perf, profiles, certs; naming conventions clarified.","status":"closed","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:41.202413409Z","updated_at":"2025-12-18T03:34:32.746769472Z","closed_at":"2025-12-18T03:34:32.746769472Z","close_reason":"Unified artifacts layout documented in artifacts/README.md; scripts/CLI now emit JSON+MD into artifacts/{baseline,bench,perf,certs} and nanochat baseline writes under artifacts/baseline/nanochat/","compaction_level":0}
{"id":"model_guided_research-vaz","title":"Per-attention-type hyperparameter sensitivity analysis","description":"Each exotic attention type may need different hyperparameters: (1) Test LR sensitivity per attention type, (2) Check batch size effects, (3) Verify gradient clipping needs, (4) Document recommended HP ranges per type, (5) Create config templates. Unfair comparison if vanilla uses optimal HPs but exotic uses defaults.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-19T20:56:20.131353005Z","updated_at":"2025-12-19T21:01:08.653887676Z","closed_at":"2025-12-19T21:01:08.653887676Z","close_reason":"Redundant with existing 2bx (Nanochat config templates for fair comparisons). HP sensitivity should be documented in config templates.","compaction_level":0}
{"id":"model_guided_research-wgd","title":"Dataset/legal/licensing sanity check","description":"- Audit datasets referenced/used by nanochat scripts for licensing/attribution; ensure cached paths and download scripts respect licensing.\n- Add a short note listing sources and constraints under docs/dataset_licenses.md; no code changes.","status":"open","priority":3,"issue_type":"chore","assignee":"","created_at":"2025-11-20T08:10:25.506292546Z","updated_at":"2025-11-20T08:10:25.506292546Z","compaction_level":0}
{"id":"model_guided_research-wiz","title":"Dataset snapshot & split pinning for CMA-ES","description":"- Define how CMA-ES runs pin dataset versions/splits to avoid silent drift; document data paths, hashes, and split seeds used in objectives.\n- Add a small helper to record dataset hash/size into artifacts per run.","acceptance_criteria":"Guideline/helper for recording dataset version/split info implemented; at least one run logs dataset hash/size into artifacts/cmaes/; documented in cmaes_plan_mgr or data note.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:34:26.000466603Z","updated_at":"2025-11-20T08:34:26.000466603Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-wiz","depends_on_id":"model_guided_research-ajh","type":"blocks","created_at":"2025-11-20T08:34:26.016554400Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-wyf","title":"Align torch/nanochat configs with bio_inspired_nanochat best practices","description":"- Compare optimizer/scheduler/compile settings (e.g., torch.compile modes, autocast/bfloat16, grad clipping, dropout, activation checkpointing) against bio_inspired_nanochat.\n- Propose concrete config changes or flags to match/improve parity while preserving mathematical features.\n- Capture findings in docs/config_parity.md with file/line pointers; defer code changes unless low-risk toggles.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:27.154069447Z","updated_at":"2025-11-20T08:08:27.154069447Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-wyf","depends_on_id":"model_guided_research-6l7","type":"blocks","created_at":"2025-11-20T08:08:27.168241521Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-x1g","title":"CA-based initializer spike","description":"- Implement a optional cellular-automaton initializer (rule30/rule116) that generates a 1D bitfield, rescales to match fan-in/out variance (Xavier/He), and reshapes to weight tensors.\n- Expose via flag/env in nanochat init utilities without touching default paths.\n- Support mixing ratio (e.g., alpha*CA + (1-alpha)*standard) and deterministic seed control.","acceptance_criteria":"Initializer function added and gated by flag/env; supports rule30/rule116, variance-correct scaling, optional mix alpha; no regression to default init; unit sanity check on variance; docs snippet added.","status":"closed","priority":2,"issue_type":"feature","assignee":"","created_at":"2025-11-20T08:25:50.957547173Z","updated_at":"2025-12-18T07:23:02.726085176Z","closed_at":"2025-12-18T07:23:02.726085176Z","close_reason":"Implemented CA initializer (rule30/rule116) with scaling, alpha mix, CLI/env wiring, variance sanity test, and README snippet.","compaction_level":0}
{"id":"model_guided_research-x8u","title":"Add optional torch.compile support to nanochat.train (esp for FlexAttention)","description":"nanochat.train currently does not torch.compile the model, so FlexAttention warns it may run an unfused path and appears slower in fixed-FLOPs baselines. Add an opt-in --compile flag (and optionally mode/backend knobs) so baseline/bench runs can fairly compare SDPA vs Flex under compilation; record compile settings in artifacts.","acceptance_criteria":"nanochat.train supports --compile (default off) on CUDA; when enabled, model forward/backward runs without errors for standard attention + --use-flex-attention; compile settings recorded in artifacts summary; tests remain green.","status":"closed","priority":2,"issue_type":"task","assignee":"","created_at":"2025-12-18T04:26:04.328970104Z","updated_at":"2025-12-18T04:48:18.741212703Z","closed_at":"2025-12-18T04:48:18.741212703Z","close_reason":"Implemented torch.compile flags in nanochat.train, compile FlexAttention callable, validated via CUDA smoke runs + pytest.","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-x8u","depends_on_id":"model_guided_research-0jk","type":"discovered-from","created_at":"2025-12-18T04:26:04.352716150Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-ybp","title":"CMA-ES for math demos (targeted knobs)","description":"- Identify 2-3 math demos where a tiny set of continuous knobs could benefit from CMA-ES (e.g., tropical sparsity/mixture lambdas, ultrametric p/packed thresholds, reversible Cayley iters, gauge structured/unstructured weights).\n- Define per-demo parameter sets/bounds and propose small-budget CMA-ES sweeps; document in a note with feasibility and costs; no runs yet.","acceptance_criteria":"Per-demo CMA-ES targets listed with 2-5 parameters each, bounds, cost estimates, and go/no-go notes; stored in docs/cmaes_math_targets.md; ready to schedule pilots.","status":"open","priority":3,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:32:27.287665873Z","updated_at":"2025-11-20T08:32:27.287665873Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-ybp","depends_on_id":"model_guided_research-ajh","type":"blocks","created_at":"2025-11-20T08:32:27.300317479Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-yhy","title":"Auto run report generator","description":"- After a harness run, generate a concise markdown (optional HTML) report summarizing config, FLOPs, seeds, metrics, math diagnostics, plots/links (vis artifacts), and pass/fail checks.\n- Hook into harness pipeline; allow manual invocation on an artifacts directory.\n- Ensure outputs land under artifacts/reports/ with unique names and provenance.","acceptance_criteria":"Report generator produces a markdown (and optional HTML) report from a harness run; includes config, FLOPs, seeds, metrics tables, key diagnostics, links to vis; saved under artifacts/reports/; can be run automatically post-run or manually on a path.","status":"open","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:18:46.228918171Z","updated_at":"2025-11-20T08:18:46.228918171Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-yhy","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:19:11.673600831Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-yhy","depends_on_id":"model_guided_research-kt8","type":"blocks","created_at":"2025-11-20T08:19:17.563240378Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-yhy","depends_on_id":"model_guided_research-uny","type":"blocks","created_at":"2025-11-20T08:19:15.091591823Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-z7r","title":"Granular A/B feature benchmarking","description":"Build a harness to swap single math features (gauge transport, ultrametric routing, tropical attn, reversible coupling, FlexAttention, etc.) into an otherwise vanilla nanochat block and compare vs vanilla under identical FLOPs, batch/seq, seeds.\n\nCollect metrics per feature:\n- Train CE and Val CE (requires 5fp)\n- tokens/s, TFLOP/s, memory\n- Instability flags (NaN/Inf counts)\n\nRun with multiple seeds (3-5) and compute:\n- Mean ± std for each metric\n- 95% CI \n- Simple significance test (Welch's t-test or similar)\n\nEmit per-feature CSV/JSON + markdown with delta vs baseline; store under artifacts/feature_ablate/. This is THE key deliverable for proving whether exotic math features beat vanilla.","acceptance_criteria":"Harness runs at least 3 features vs vanilla with matched configs; outputs structured JSON/CSV + markdown deltas; seeds logged; FLOPs budget documented; results reproducible via recorded commands.","status":"open","priority":1,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:14:28.585135751Z","updated_at":"2025-12-19T21:01:42.911914405Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-z7r","depends_on_id":"model_guided_research-2e5","type":"blocks","created_at":"2025-11-20T08:14:57.891033068Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-z7r","depends_on_id":"model_guided_research-5bo","type":"blocks","created_at":"2025-11-20T08:14:54.749157396Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-z7r","depends_on_id":"model_guided_research-5fp","type":"blocks","created_at":"2025-12-19T21:01:28.517922123Z","created_by":"daemon","metadata":"{}","thread_id":""},{"issue_id":"model_guided_research-z7r","depends_on_id":"model_guided_research-gjm","type":"blocks","created_at":"2025-11-20T08:14:52.023886809Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
{"id":"model_guided_research-zg1","title":"Restore and enhance inline comments across nanochat core","description":"- Pass through nanochat/* (synaptic.py, gpt*.py, attention modules, optimizers, scheduler) to reintroduce explanatory comments that were removed; add brief clarifying comments where logic is dense.\n- Avoid noise; ensure comments align with docs/math; no functional changes.\n- Track progress by file; ensure net comment gain where applicable.","status":"in_progress","priority":2,"issue_type":"task","assignee":"","created_at":"2025-11-20T08:08:30.392018704Z","updated_at":"2025-12-19T04:25:45.844182632Z","compaction_level":0,"dependencies":[{"issue_id":"model_guided_research-zg1","depends_on_id":"model_guided_research-3bx","type":"blocks","created_at":"2025-11-20T08:08:30.393420603Z","created_by":"daemon","metadata":"{}","thread_id":""}]}
