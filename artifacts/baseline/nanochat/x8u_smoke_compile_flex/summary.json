{
  "argv": [
    "/data/projects/model_guided_research/nanochat/train.py",
    "--device",
    "cuda",
    "--batch-size",
    "2",
    "--sequence-len",
    "64",
    "--n-layer",
    "2",
    "--n-head",
    "4",
    "--n-kv-head",
    "4",
    "--n-embd",
    "64",
    "--max-steps",
    "3",
    "--warmup-steps",
    "1",
    "--log-interval",
    "1",
    "--run-id",
    "x8u_smoke_compile_flex",
    "--use-flex-attention",
    "--compile",
    "--compile-mode",
    "default"
  ],
  "budget": {
    "flops_per_step_est": 2560622592,
    "flops_per_token_est": 20004864,
    "max_steps": 3,
    "planned_total_flops_est": 7681867776,
    "target_flops": null,
    "tokens_per_step_global": 128,
    "warmup_steps": 1
  },
  "command": "uv run python -m nanochat.train --device cuda --batch-size 2 --sequence-len 64 --n-layer 2 --n-head 4 --n-kv-head 4 --n-embd 64 --max-steps 3 --warmup-steps 1 --log-interval 1 --run-id x8u_smoke_compile_flex --use-flex-attention --compile --compile-mode default",
  "command_argv": "/data/projects/model_guided_research/nanochat/train.py --device cuda --batch-size 2 --sequence-len 64 --n-layer 2 --n-head 4 --n-kv-head 4 --n-embd 64 --max-steps 3 --warmup-steps 1 --log-interval 1 --run-id x8u_smoke_compile_flex --use-flex-attention --compile --compile-mode default",
  "command_module": "python -m nanochat.train --device cuda --batch-size 2 --sequence-len 64 --n-layer 2 --n-head 4 --n-kv-head 4 --n-embd 64 --max-steps 3 --warmup-steps 1 --log-interval 1 --run-id x8u_smoke_compile_flex --use-flex-attention --compile --compile-mode default",
  "compile": {
    "backend": "inductor",
    "compile_flex_attention": true,
    "dynamic": null,
    "enabled": true,
    "fullgraph": false,
    "mode": "default"
  },
  "config": {
    "attention_type": "standard",
    "compile_backend": "inductor",
    "compile_dynamic": null,
    "compile_flex_attention": true,
    "compile_fullgraph": false,
    "compile_mode": "default",
    "n_embd": 64,
    "n_head": 4,
    "n_kv_head": 4,
    "n_layer": 2,
    "optimizer_type": "adamw",
    "sequence_len": 64,
    "use_flex_attention": true,
    "vocab_size": 50304
  },
  "dataset": {
    "parquet_files": [
      "/home/ubuntu/.cache/nanochat/base_data/shard_00000.parquet",
      "/home/ubuntu/.cache/nanochat/base_data/shard_00001.parquet"
    ],
    "parquet_files_count": 2
  },
  "ddp": {
    "enabled": false,
    "local_rank": 0,
    "rank": 0,
    "world_size": 1
  },
  "env": {
    "TORCHINDUCTOR_CACHE_DIR": "/tmp/torchinductor_ubuntu"
  },
  "git": {
    "branch": "main",
    "commit": "95ab717",
    "commit_full": "95ab717124a53d816acfdc82d21628e443b811df",
    "dirty": true,
    "message": "Comprehensively revise README.md to reflect dual-implementation architecture"
  },
  "gpu": {
    "available": true,
    "count": 2,
    "cuda_version": "12.8",
    "memory_gb": [
      23.5164794921875,
      23.5135498046875
    ],
    "names": [
      "NVIDIA GeForce RTX 4090",
      "NVIDIA GeForce RTX 4090"
    ]
  },
  "python": {
    "executable": "/data/projects/model_guided_research/.venv/bin/python3",
    "version": "3.13.0 (main, Oct 16 2024, 03:23:02) [Clang 18.1.8 ]"
  },
  "results": {
    "losses": [
      10.82583999633789,
      10.826264381408691,
      10.811397552490234
    ],
    "measured_steps": 2,
    "measured_time_s": 0.056301749020349234,
    "measured_tokens": 256,
    "peak_memory_allocated_gb": 0.1613459587097168,
    "step_times_s": [
      14.691853185999207,
      0.030139720009174198,
      0.02338098204927519
    ],
    "tflops_per_second_est": 0.09096067658837775,
    "tokens_per_second": 4546.9280165252685
  },
  "run_id": "x8u_smoke_compile_flex",
  "system": {
    "cpu_count": 32,
    "cpu_count_logical": 64,
    "hostname": "threadripperje",
    "memory_gb": 499.2872734069824,
    "nanochat_base_dir": "/home/ubuntu/.cache/nanochat",
    "nanochat_base_dir_env": null,
    "platform": "Linux",
    "python_version": "3.13.0",
    "torch_version": "2.9.1+cu128",
    "user": "ubuntu",
    "working_dir": "/data/projects/model_guided_research"
  }
}
